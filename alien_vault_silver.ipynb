{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc8a702",
   "metadata": {},
   "source": [
    "References to the documentation describing the structure of the JSON objects:  [AlienVault Object](https://otx.alienvault.com/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b8897fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b7306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alien_vault_batch_1.ndjson',\n",
       " 'alien_vault_batch_3.ndjson',\n",
       " 'alien_vault_batch_2.ndjson']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"bronze/alien_vault\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cee4457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/06 12:55:31 WARN Utils: Your hostname, MacBook-Pro-de-Macia.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.138 instead (on interface en0)\n",
      "25/09/06 12:55:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/06 12:55:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/06 12:55:33 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: bronze/alien_vault/*.ndjson.\n",
      "java.io.FileNotFoundException: File bronze/alien_vault/*.ndjson does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    }
   ],
   "source": [
    "# Init Spark\n",
    "spark = SparkSession.builder.appName(\"AlienVaultIngest\").getOrCreate()\n",
    "\n",
    "# Load alien vault jsons\n",
    "df_raw = spark.read.json(\"bronze/alien_vault/*.ndjson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a194ae9",
   "metadata": {},
   "source": [
    "# Creating the Dataset from AlienVault Response\n",
    "\n",
    "In this notebook, we are building a structured dataset based on the raw data returned by **AlienVault**.  \n",
    "The raw data comes in JSON format, where each record contains a `\"response\"` field with multiple elements. Each element includes information such as the threat `name`, `description`, `TLP` (Traffic Light Protocol), associated `tags`, and `malware_families`.  \n",
    "\n",
    "We first define a **schema** for the elements inside `\"response\"` and for the overall JSON structure. Then, we read all the `.ndjson` files from the Bronze layer using this schema.  \n",
    "\n",
    "After loading the raw data, we **flatten** the nested `\"response\"` array using the `explode` function and aggregate the fields by `id` and `file_extracted`. This results in a dataset where all information related to each file is collected in lists, making it easier to analyze and process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff65019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/06 12:55:34 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: bronze/alien_vault/*.ndjson.\n",
      "java.io.FileNotFoundException: File bronze/alien_vault/*.ndjson does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='0.client-channel.google.com', file_extracted='/app/ingestion/../data_to_extract/white_list_domain.txt', names=['23.219.89.169  dty-274d7ae9-e5e0-48eb-80db-f8daf26a8d1b-default-prod-64333_23af6e1645db3b350058.js', 'recaptcha__pl.js', 'Cyber Army', 'Project Skynet '], descriptions=['https://www.virustotal.com/gui/file/191475f518a3563c8bbe32e742cc9106c0c968e2e3b9ce12aa12b5f018cbac42/relations\\nhttps://www.virustotal.com/gui/ip-address/23.219.89.169/relations', '', '174.bm-nginx-loadbalancer.mgmt.sin1.adnexus.net\\nCyber Army, Project Skynet, malware, malware site, Deobfuscate/Decode Files or Information\\nAttacking www songcultre.com found in  malicious cloudfront.net link.', ''], TLPs=['white', 'white', 'white', 'white'], tags=[], malware_families=[[], [], [], []]),\n",
       " Row(id='17track.net', file_extracted='/app/ingestion/../data_to_extract/white_list_domain.txt', names=['Remote Network Attack | JakyllHyde: Malicious Keyword Tool Index | Sabey Data Centers'], descriptions=[\"Research shows compromise originated from Sabey Data Centers. High Priority 'Malicious' \\nRemotely connects to victim network is injection,\"], TLPs=['green'], tags=[], malware_families=[[]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BooleanType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Schema for the elements inside response\n",
    "response_element_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"TLP\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"Tags\", ArrayType(StringType()), True),\n",
    "    StructField(\"malware_families\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "response_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"file_extracted\", StringType(), True),\n",
    "    StructField(\"response\", ArrayType(response_element_schema), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_raw = spark.read.schema(response_schema).json(\"bronze/alien_vault/*.ndjson\")\n",
    "\n",
    "df_flat = (\n",
    "    df_raw\n",
    "    .withColumn(\"response\", explode(col(\"response\")))\n",
    "    .groupBy(\"id\", \"file_extracted\")\n",
    "    .agg(\n",
    "        F.collect_list(col(\"response.name\")).alias(\"names\"),\n",
    "        F.collect_list(col(\"response.description\")).alias(\"descriptions\"),\n",
    "        F.collect_list(col(\"response.TLP\")).alias(\"TLPs\"),\n",
    "        F.collect_list(col(\"response.tags\")).alias(\"tags\"),\n",
    "        F.collect_list(col(\"response.malware_families\")).alias(\"malware_families\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_flat.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8880b",
   "metadata": {},
   "source": [
    "# Assigning Threat Status Based on File Source\n",
    "\n",
    "In this step, we are creating a new column called `threat_status` to classify each file based on its source.  \n",
    "\n",
    "- If the `file_extracted` path contains `\"black_list\"`, the file is labeled as **\"malicious\"**.  \n",
    "- If it contains `\"white_list\"`, it is labeled as **\"whitelist\"**.  \n",
    "- Otherwise, the status is set to **\"unknown\"**.  \n",
    "\n",
    "This allows us to quickly categorize files and assess potential threats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a435e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_flat = df_flat.withColumn(\n",
    "    \"threat_status\",\n",
    "    F.when(F.col(\"file_extracted\").contains(\"black_list\"), \"malicious\")\n",
    "     .when(F.col(\"file_extracted\").contains(\"white_list\"), \"whitelist\")\n",
    "     .otherwise(\"unknown\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f574fe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'names', 'descriptions', 'TLPs', 'tags', 'malware_families',\n",
       "       'threat_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df_flat.toPandas()\n",
    "df_final = df_final.drop('file_extracted', axis=1)\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bbe47a",
   "metadata": {},
   "source": [
    "# Cleaning and Preparing Columns for Analysis\n",
    "\n",
    "In this step, we are processing and cleaning some of the key columns in `df_final` to prepare the data for further analysis, such as Natural Language Processing (NLP) models.\n",
    "\n",
    "- **TLPs**: We remove duplicate entries from the `TLPs` lists by converting each list into a dictionary (to keep only unique values) and then back to a list. This results in an array of unique TLP categories for each record.\n",
    "  \n",
    "- **names**: Multiple names associated with each file are combined into a single string, joining them with spaces. This makes it easier to feed the data into NLP models.\n",
    "\n",
    "- **descriptions**: Similarly, multiple descriptions are joined into a single string. Additionally, any URLs (starting with http://, https://, or www.) are excluded to keep only textual information relevant for analysis.\n",
    "\n",
    "This preprocessing ensures that names and descriptions are ready as clean text inputs for NLP models, while TLPs remains a structured array of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab46a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['TLPs'] = df_final['TLPs'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "\n",
    "df_final[\"names\"] = df_final[\"names\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "df_final[\"descriptions\"] = df_final[\"descriptions\"].apply(\n",
    "    lambda x: \" \".join([w for w in x if not w.startswith((\"http://\", \"https://\", \"www.\"))])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3288b9",
   "metadata": {},
   "source": [
    "As we have seen, `tags` and `malware_families` are always empty, so we can remove them to obtain a cleaner dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c6f276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>names</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>TLPs</th>\n",
       "      <th>threat_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.client-channel.google.com</td>\n",
       "      <td>23.219.89.169  dty-274d7ae9-e5e0-48eb-80db-f8d...</td>\n",
       "      <td>174.bm-nginx-loadbalancer.mgmt.sin1.adnexus.n...</td>\n",
       "      <td>[white]</td>\n",
       "      <td>whitelist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17track.net</td>\n",
       "      <td>Remote Network Attack | JakyllHyde: Malicious ...</td>\n",
       "      <td>Research shows compromise originated from Sabe...</td>\n",
       "      <td>[green]</td>\n",
       "      <td>whitelist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1drv.com</td>\n",
       "      <td>DarkWatchman Chekin Activity Order  Brian Sabe...</td>\n",
       "      <td>Brian Sabey &amp; large team continue excessive ...</td>\n",
       "      <td>[green]</td>\n",
       "      <td>whitelist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25z5g623wpqpdwis.onion.to</td>\n",
       "      <td>IOC Records Provided by @NextRayAI IOCs Indust...</td>\n",
       "      <td>This IOC report provided and daily updated by ...</td>\n",
       "      <td>[white]</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27lelchgcvs2wpm7.3lhjyx.top</td>\n",
       "      <td>TomkompSerwis   5b685b6fd0c356b8389e33596a40c6...</td>\n",
       "      <td>Dŵr dysku zewnętrznego wedi cymryd i'wodraeth ...</td>\n",
       "      <td>[white]</td>\n",
       "      <td>malicious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id  \\\n",
       "0  0.client-channel.google.com   \n",
       "1                  17track.net   \n",
       "2                     1drv.com   \n",
       "3    25z5g623wpqpdwis.onion.to   \n",
       "4  27lelchgcvs2wpm7.3lhjyx.top   \n",
       "\n",
       "                                               names  \\\n",
       "0  23.219.89.169  dty-274d7ae9-e5e0-48eb-80db-f8d...   \n",
       "1  Remote Network Attack | JakyllHyde: Malicious ...   \n",
       "2  DarkWatchman Chekin Activity Order  Brian Sabe...   \n",
       "3  IOC Records Provided by @NextRayAI IOCs Indust...   \n",
       "4  TomkompSerwis   5b685b6fd0c356b8389e33596a40c6...   \n",
       "\n",
       "                                        descriptions     TLPs threat_status  \n",
       "0   174.bm-nginx-loadbalancer.mgmt.sin1.adnexus.n...  [white]     whitelist  \n",
       "1  Research shows compromise originated from Sabe...  [green]     whitelist  \n",
       "2    Brian Sabey & large team continue excessive ...  [green]     whitelist  \n",
       "3  This IOC report provided and daily updated by ...  [white]     malicious  \n",
       "4  Dŵr dysku zewnętrznego wedi cymryd i'wodraeth ...  [white]     malicious  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = df_final.drop(['tags', 'malware_families'], axis=1)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce32c72",
   "metadata": {},
   "source": [
    "Ffinally saving the `Silver` dataset for `alien_vault`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc03102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('silver/alien_vault/alien_vault.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012c7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3024 entries, 0 to 3023\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             3024 non-null   object\n",
      " 1   names          3024 non-null   object\n",
      " 2   descriptions   3024 non-null   object\n",
      " 3   TLPs           3024 non-null   object\n",
      " 4   threat_status  3024 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 118.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
